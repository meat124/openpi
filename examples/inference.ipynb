{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/hyunjin/rby1_ws/openpi/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n"
          ]
        }
      ],
      "source": [
        "import dataclasses\n",
        "import gc\n",
        "\n",
        "import jax\n",
        "\n",
        "from openpi.models import model as _model\n",
        "from openpi.policies import rby1_policy\n",
        "from openpi.policies import policy_config as _policy_config\n",
        "from openpi.shared import download\n",
        "from openpi.training import config as _config\n",
        "from openpi.training import data_loader as _data_loader\n",
        "\n",
        "\n",
        "def _cleanup():\n",
        "    \"\"\"Best-effort cleanup after exceptions.\n",
        "\n",
        "    Notes:\n",
        "    - JAX/XLA may keep GPU memory reserved for reuse; this still helps release Python refs\n",
        "      and clear compilation caches.\n",
        "    - If you're running a long notebook session and hit OOM, a kernel restart is the\n",
        "      most reliable way to fully reset GPU memory.\n",
        "    \"\"\"\n",
        "\n",
        "    # Drop Python references.\n",
        "    gc.collect()\n",
        "\n",
        "    # Clear JAX compilation/executable caches.\n",
        "    try:\n",
        "        jax.clear_caches()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # If torch is present (e.g., PyTorch checkpoints), clear its CUDA cache.\n",
        "    try:\n",
        "        import torch\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "    except Exception:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Policy inference\n",
        "\n",
        "The following example shows how to create a policy from a checkpoint and run inference on a dummy example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actions shape: (50, 16)\n"
          ]
        }
      ],
      "source": [
        "config = _config.get_config(\"pi05_rby1\")\n",
        "checkpoint_dir = download.maybe_download(\"../checkpoints/pi05_rby1/pi05_rby1_test/200\")\n",
        "# checkpoint_dir = \"../checkpoints/pi05_rby1/pi05_rby1_test/200\"\n",
        "policy = None\n",
        "result = None\n",
        "\n",
        "try:\n",
        "    # Create a trained policy.\n",
        "    policy = _policy_config.create_trained_policy(config, checkpoint_dir)\n",
        "\n",
        "    # Run inference on a dummy example.\n",
        "    example = rby1_policy.make_rby1_example()\n",
        "    result = policy.infer(example)\n",
        "\n",
        "    print(\"Actions shape:\", result[\"actions\"].shape)\n",
        "finally:\n",
        "    # Always clean up even if an exception occurs.\n",
        "    try:\n",
        "        del policy\n",
        "    except NameError:\n",
        "        pass\n",
        "    try:\n",
        "        del result\n",
        "    except NameError:\n",
        "        pass\n",
        "    _cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Working with a live model\n",
        "\n",
        "\n",
        "The following example shows how to create a live model from a checkpoint and compute training loss. First, we are going to demonstrate how to do it with fake data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss shape: (1, 50)\n"
          ]
        }
      ],
      "source": [
        "config = _config.get_config(\"pi05_rby1\")\n",
        "\n",
        "checkpoint_dir = download.maybe_download(\"../checkpoints/pi05_rby1/pi05_rby1_test/200\")\n",
        "key = jax.random.key(0)\n",
        "\n",
        "model = None\n",
        "obs = act = loss = None\n",
        "\n",
        "try:\n",
        "    # Create a model from the checkpoint.\n",
        "    model = config.model.load(_model.restore_params(checkpoint_dir / \"params\"))\n",
        "\n",
        "    # We can create fake observations and actions to test the model.\n",
        "    obs, act = config.model.fake_obs(), config.model.fake_act()\n",
        "\n",
        "    # Compute loss.\n",
        "    loss = model.compute_loss(key, obs, act)\n",
        "    print(\"Loss shape:\", loss.shape)\n",
        "finally:\n",
        "    try:\n",
        "        del model\n",
        "    except NameError:\n",
        "        pass\n",
        "    try:\n",
        "        del obs, act, loss\n",
        "    except NameError:\n",
        "        pass\n",
        "    _cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we are going to create a data loader and use a real batch of training data to compute the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reduce the batch size to reduce memory usage.\n",
        "config = dataclasses.replace(config, batch_size=2)\n",
        "\n",
        "loader = None\n",
        "obs = act = loss = None\n",
        "\n",
        "try:\n",
        "    # Load a single batch of data. This is the same data that will be used during training.\n",
        "    # NOTE: In order to make this example self-contained, we are skipping the normalization step\n",
        "    # since it requires the normalization statistics to be generated using `compute_norm_stats`.\n",
        "    loader = _data_loader.create_data_loader(config, num_batches=1, skip_norm_stats=True)\n",
        "    obs, act = next(iter(loader))\n",
        "\n",
        "    loss = model.compute_loss(key, obs, act)\n",
        "    print(\"Loss shape:\", loss.shape)\n",
        "finally:\n",
        "    try:\n",
        "        del loader\n",
        "    except NameError:\n",
        "        pass\n",
        "    try:\n",
        "        del obs, act, loss\n",
        "    except NameError:\n",
        "        pass\n",
        "    # `model` is created in the previous cell; we still clean caches here.\n",
        "    _cleanup()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
